Below is a **clean, properly formatted single Markdown document** that compiles the entire final PPT you created into a polished written report.

You can export this as **Markdown → PDF**, share it internally, or paste it into Confluence/Notion as is.

---

# **Automating Regulatory → Internal Requirement Mapping**

*A technical deep-dive into retrieval experiments, their limitations, and a proposed agentic full-space reasoning approach*

---

## **Executive Summary**

Over the past few weeks, I’ve been working on analyzing and improving the automation of mapping external regulatory citations (NIST, DORA, ISO, etc.) to our internal cybersecurity requirements.

This document summarizes:

1. **The core problem we face**
2. **The retrieval-based approaches tried previously by the team and the enhancements I experimented with**
3. **Why retrieval—even in advanced forms—does not fully solve this use case**
4. **A proposed architecture using domain-specialist agents who evaluate the entire requirement space**

The earlier experiments made it clear that retrieval alone had precision and recall constraints in this scenario. I used those insights as a starting point for further exploration and to evaluate alternative approaches.

---

## **The Core Problem**

Our organization maintains a large set of internal cybersecurity requirements (around 4,000). These internal requirements are usually:

* **Very concrete**,
* **Operational**, and
* **Implementation-focused**

They describe *what exactly needs to happen*, *who must do it*, and *how often it must be done*.

External regulatory citations—whether from NIST, DORA, ISO, or other bodies—tend to be:

* **High-level**,
* **Principle-driven**,
* **Abstract**, and
* **Intent-focused**

They express what regulators expect but rarely specify the mechanisms needed to achieve those outcomes.

This creates a structural challenge:

> **The language, abstraction level, and intent of regulatory citations rarely match how our internal controls are written.**

Below are detailed examples illustrating these mismatches.

---

## **Example 1: Governance Alignment vs Process Ownership**

**External Citation**

> “The organization shall ensure appropriate oversight and alignment of cybersecurity activities with its strategic objectives.”

**Internal Requirement**

> “Risk owners must be assigned to each business process, and review meetings must be held quarterly.”

**Why Retrieval Fails**

* Citation uses: *oversight*, *alignment*, *strategic objectives*
* Requirement uses: *risk owners*, *processes*, *review meetings*

There is almost no lexical or semantic overlap. Dense and sparse retrieval fail to connect them.

**But logically:** Assigning risk owners + governance reviews *is exactly how alignment is achieved*.

---

## **Example 2: Timely Incident Response vs Tiered Escalation**

**External Citation**

> “Organizations must be prepared to detect and respond to cybersecurity incidents in a timely manner.”

**Internal Requirement**

> “The SOC shall escalate incidents to Tier-2 analysts within 15 minutes based on predefined severity thresholds.”

**Retrieval Problems**
Citation uses broad terms (*prepared*, *detect*, *respond*, *timely*).
Requirement uses operational terms (*SOC*, *Tier-2*, *15 minutes*).

They appear unrelated in embedding space, but operationally the internal control is the exact enforcement of the citation’s intent.

---

## **What We Have Built So Far (and What I Learned From It)**

Before I joined this effort, the team implemented a solid RAG-based model:

* Dense embeddings over requirement chunks
* Vector store + similarity retrieval
* LLM-based ranking of retrieved candidates

This work helped me understand:

* How internal requirements are structured
* How regulatory citations vary
* Where retrieval tends to break
* How precision and recall behave in practice

The team also experimented with:

* Adding contextual metadata (Domain, Topic, etc.)
* RAG Fusion (sub-citations generated by an LLM)

These improved recall somewhat but still missed many valid mappings.

This provided a strong foundation for my next experiments.

---

## **Retrieval Techniques I Explored**

To push retrieval as far as possible, I tried:

1. **Sparse Neural Retrieval** (SPLADE / ColBERT style)
2. **Hybrid Retrieval** (dense + sparse fusion)
3. **Reciprocal Rank Fusion (RRF)**
4. **Deep-dive into RAG Fusion**

These techniques are state-of-the-art in many QnA and search contexts.
However, evaluations showed the same core limitation: retrieval alone is not enough.

---

## **Dense Embedding Retrieval: Why It Fell Short**

Dense embeddings excel at measuring semantic similarity but fail when:

* Two texts are conceptually linked
* But have no semantic proximity

Example:

**Citation:**

> “Cybersecurity must align with organizational vision and leadership.”

**Requirement:**

> “Risk owners must be assigned to each business process.”

Dense retrieval cannot infer:
**role assignment → governance → oversight → strategic alignment**

Thus, dense retrieval misses mechanistically relevant but textually unrelated requirements.

---

## **Sparse Neural Retrieval: What It Adds & Why It Still Misses**

### **What Sparse Models Do**

Sparse neural retrieval (e.g., SPLADE):

* Behaves like a **smarter BM25**
* Produces sparse vectors weighted by token importance
* Expands relevant terms
* Down-weights noise
* Improves keyword-based retrieval

### **Why I Tried It**

Dense models were sometimes *too fuzzy*, blurring domain-specific terminology.
Sparse models are excellent when terminology matters.

### **Where It Breaks**

Sparse models rely on **token overlap** or **token-level relatedness**.
Regulatory and internal texts rarely share vocabulary.

Sparse retrieval cannot infer:

* “risk owner assignment” → “oversight”
* “15-min escalation” → “timely incident response”

### **Core Limitation**

They operate in **lexical space**, not **reasoning space**.
They cannot bridge abstraction gaps.

---

## **Hybrid Retrieval + RRF: Strong Retrieval, Still Insufficient**

Hybrid retrieval blends:

* **Dense embeddings** → conceptual meaning
* **Sparse signals** → term precision

RRF (Reciprocal Rank Fusion) merges ranked lists and often produces excellent search performance in:

* document search
* FAQ systems
* legal search
* QnA
* enterprise search

### **Why I Tried It**

To increase candidate diversity so the LLM reranker could evaluate a richer pool.

### **What Worked**

* More diverse candidates
* Sparse models recovered items dense models missed
* RRF stabilized rankings
* Great performance for QnA-style tasks

### **Where It Still Failed**

Even the best hybrid retrieval depends on *similarity signals*.

The fundamental issue:

> When dense and sparse retrieval both cannot see a conceptual relationship, RRF has nothing meaningful to fuse.

Thus—even the most advanced hybrid retrieval misses many valid mappings.

---

## **Why Retrieval (In Any Form) Will Always Underperform**

This insight emerged clearly:

**Retrieval methods operate on textual and semantic similarity.
Our mapping task requires conceptual, organizational, and multi-hop reasoning.**

Example reasoning chain retrieval cannot replicate:

1. The citation requires **governance alignment**
2. Governance alignment is established via **assigning accountable owners**
3. Requirement R-001 assigns **risk owners**
4. Therefore, R-001 helps **satisfy** the citation

Retrieval cannot perform such reasoning.
It can only find “nearby” sentences, not infer implicit relationships.

---

## **Proposed Direction: Full-Space, Domain-Specialist Agents**

Given all findings, the logical next step is to shift from retrieval-driven filtering to **exhaustive reasoning**.

### **The idea:**

* Build specialist agents (Risk, Access Control, Incident Response, etc.)
* Give each agent the *entire* requirement set for its domain
* Let them evaluate requirements logically, in batches
* Use stepwise, structured LLM reasoning instead of similarity lookup
* Run agents in parallel to maintain throughput

This ensures:

> **Every requirement is evaluated, not just the ones that appear semantically similar.**

---

## **Why “Brute Force” Is Acceptable (and Preferred)**

Although computationally heavier, this approach aligns with how compliance risk should be handled:

* **Accuracy > speed**
* **Missing a requirement is worse than taking longer**
* **Full auditability**: every requirement reviewed
* Over time we can optimize with batching, caching, heuristics, etc.

---

## **Orchestration & Concurrency**

To maintain throughput:

* Batch requirements (50–200 per batch)
* Parallelize domain agents
* Add rate limiting & checkpointing
* Cache repeated evaluations

Concurrency enables exhaustive coverage without unacceptable latency.

---

## **Conclusion**

After exploring dense embeddings, sparse neural retrieval, hybrid models, RRF fusion, reranking strategies, and building on the earlier RAG experiments:

### **The conclusion is both empirical and conceptual:**

**RAG (or any retrieval-centric approach) is not suitable as the core solution for this type of regulatory mapping.**

Why?

* Regulatory citations express **high-level intent**
* Internal requirements express **operational mechanisms**
* Their relationship is **implicit, multi-hop, and organizational** — not textual
* Retrieval focuses on **similarity**, whereas the task requires **reasoning**

Even with the best retrieval models, the structural mismatch guarantees high false negatives.

### **Most importantly:**

> **In compliance, missing a requirement is far more costly than time or computation.**
>
> Therefore, we must allow LLMs to evaluate *more* requirements, not restrict them via similarity filtering.

This points toward an **agentic, full-space, domain-specialist reasoning approach**, where LLMs systematically analyze the entire requirement corpus.

This approach will evolve with optimizations—batching, concurrency, caching—but directionally it aligns with the core nature of the problem:
**exhaustive reasoning, not similarity retrieval, is what delivers the accuracy and coverage compliance demands.**

---

If you'd like, I can also generate:
✅ A PDF-ready version
✅ A more formal whitepaper layout
✅ A shorter executive summary

Just tell me.
